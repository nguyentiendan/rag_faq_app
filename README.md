# RAG FAQ Application

A Retrieval-Augmented Generation (RAG) based FAQ system that allows users to upload documents (PDF, DOC/DOCX) and ask questions via a chat interface.

Built with **Next.js**, **FastAPI**, and **LangChain**.

---

## âœ¨ Features

- ğŸ“„ Upload PDF / DOC / DOCX documents
- ğŸ’¬ Chat-based Q&A over uploaded documents
- ğŸ” Semantic search with vector embeddings
- ğŸ§  Context-aware answers using LLMs
- âš¡ Fast, local vector storage

---

## ğŸ— Architecture Overview

```
Frontend (Next.js)
      â”‚
      â”‚  Upload / Chat
      â–¼
Backend API (FastAPI)
      â”‚
      â”‚  RAG Pipeline
      â–¼
LangChain
  â”œâ”€ Document Loader
  â”œâ”€ Text Splitter
  â”œâ”€ Embeddings
  â””â”€ Retriever
      â”‚
      â–¼
Vector Store (ChromaDB / FAISS)
      â”‚
      â–¼
LLM (OpenAI / Gemini)
```

![Architecture Diagram](images/diagram.jpg)

![System flow & Sequence Diagram](images/flow.jpg)
---

## ğŸ§° Tech Stack

**Frontend**
- Next.js
- React

**Backend**
- FastAPI
- LangChain
- ChromaDB / FAISS

**LLM Providers**
- OpenAI
- Gemini (optional)

---

## ğŸ“¦ Prerequisites

- Node.js v18+
- Python v3.9+
- OpenAI API Key

---

## ğŸ“ Project Structure

```
rag_faq_app/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ chroma_db/
â”‚   â”œâ”€â”€ uploads/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â””â”€â”€ frontend/
    â”œâ”€â”€ app/
    â”œâ”€â”€ components/
    â””â”€â”€ package.json
```

---

## ğŸš€ Getting Started

### Backend

```bash
cd rag_faq_app/backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload
```

### Frontend

```bash
cd rag_faq_app/frontend
npm install
npm run dev
```

---

## ğŸ§ª Usage

1. Upload document
2. Ask questions
3. Get grounded answers

---
## ğŸ§ª Notes

1. The vector database is stored locally in rag_faq_app/backend/chroma_db.
2. Uploaded files are saved in rag_faq_app/backend/uploads.

---

## Need to improve
- Metadata Filtering: Filter retrieval by filename/date.
- Hybrid Search: Combine keyword search (BM25) with semantic search.
- History Aware: Pass chat history to the LLM for context-aware follow-up questions.
- Async Processing: Use background tasks (Celery/RQ) for processing large files to avoid blocking the API.
- Source Highlighting: Return page numbers and highlight text in the UI.

---

## ğŸ“„ License

MIT
